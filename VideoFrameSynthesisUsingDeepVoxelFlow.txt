【Abstract】
既存のビデオの新しいビデオフレームを合成する問題に対処します。既存のフレーム間（補間）、またはそれらの後続（補外）のいずれかです。 ビデオの外観とモーションは非常に複雑になる可能性があるため、この問題は困難です。 従来のオプティカルフローベースのソリューションは、フローの推定が困難な場合に失敗することが多く、ピクセル値を直接幻覚化する新しいニューラルネットワークベースの方法では、ぼやけた結果がしばしば発生します。 ディープボクセルフローと呼ばれる既存のピクセル値からピクセル値を流してビデオフレームを合成することを学習するディープネットワークをトレーニングすることにより、これら2つの方法の利点を組み合わせます。 私たちの方法は、人間の監督を必要とせず、既存のフレームをドロップしてから予測することを学習することで、ビデオをトレーニングデータとして使用できます。 この手法は効率的であり、任意のビデオ解像度で適用できます。
私たちの方法が、最新技術を定量的および定性的に改善する結果を生み出すことを実証します



【1．Introduction】
一般的なシーンのビデオでは複雑な現象の集合が観測できる。
物体が分離したり、速く動いたり、お互いが隠れたり現れたり、シーンのライティングが変わったり、


3.1 Deep Voxel Flow(DVF)
我々が提案するDVFは動画フレーム合成についてend-to-endで完全に微分可能なネットワークである。
トレーニングデータとして必要なのは連続した3つの動画フレームである。
トレーニングプロセスにおいて、2つのフレームを入力都市、残りのフレームをターゲットフレームとする。
我々のアプローチはself-supervisedであり、近くのフレームからvoxelを借りてくることによって再構成するように学習する。
そうすることでゼロからピクセルを生成する技術よりもよりリアルでシャープな結果となる（図4）
さらに、フレキシブルなモーションモデリングであるため、入力データの前処理(例えばpre-alignmentやlighting adjustment)を必要としない。

図1では、DVFのパイプライン処理を記載している。
Convolutional endocer-decoderが3D voxel Flowを推論し、それに応じてvolume sampling layerが目的のフレームを合成する。

DVFは合成のターゲットフレームY∈R^H*Wを入力フレームX∈R^H*W*Lから学習する。
なお、H,W,Lは入力ビデオの高さ、幅、フレーム数である。

たーげとフレームYは補間や外挿になりうる。
簡単のために外挿に焦点を絞ってL=2とする。



4.Experiments
Competing Methods.
我々のアプローチに対して　state-of-the-art オプティカルフロー技術であるEpicFlow[25]を含むいくつかの手法を比較した。
与えられた画像と計算されたflow fieldを合成するために、Middlebury interpolation benchmarkで使用されているinterpolationアルゴリズムを適用した。[1]
CNNに基づいた手法では、ビデオの推論においてより良いパフォーマンスを達成しているかどうかをDVFとMSEで比較する。[21]
しかし名がr、これらの手法では4つのフレームの入力に対して学習されている一方で、我々の手法では2つのフレームである。
したがって、我々はどちらの入力数に対しても挑戦してみている。
比較手法では、2つの設定において演算を行っている。
一つは、我々がベストなパフォーマンスとなった損失関数(ADV+GDL)を用い、Beyond MSEのバックボーンネットワークを我々のネットワークに置き換え、DVFと同じデータとプロトコルを使用して学習する。
（例えば、UCF-101の2フレームを入力とする。）
二つ目は、DVFにそれらの設定を適用させ、[21]で報告される数値に対して直接ベンチマークする。
（例えば、入力に4フレームを使用し、同じ数値のネットワークパラメータを適用する。）

【結果】
表１の左図に示すように、我々の手法はビデオの保管に関して基準よりも優れている。
BeyondMSEはハルキネーションに基づいた手法であり、ぼやけた画像を生成している。
EpicFlowは、予測したflow fieldを基にしたピクセルコピーであるため、BeyondMSEを1.4dB上回っている。
我々のアプローチは1.6dB改善しており、大きく改善している。
図4(a)にいくつかの定性的ひかくけっかを示す。

ビデオの外挿結果については表1の中央に示す。
このタスクは、ディープモデルの強みであるより意味論的な推論を要求されるためBeyond MSEとEpicFlowの間の差は0.7dBに縮まっている。
我々のアプローチは両社の利点を組み合わせ、ベストなパフォーマンスを達成した(32.7dB)。
図4(c)に定性的な比較を示す。

最後に、我々はmulti-step predictionの可能性について説明する。例えば、1フレームではなく、3つのフレームの内挿や外挿することを考える。

図5より、我々のアプローチは一貫して他の代替手法よりもすべてのステップにおいて上回っている。
有効性は長い範囲の予測における評価においてより大きくなっている。(例えば、step=3の外挿)。
DVFは大規模な教師なし学習により、長期的な時間依存性を学習できる。
定性的な評価について図4(b)(d)に示す

4.1　Multi-scale Voxel Flowの有効性
このセクションでは、Multi-scale Voxel Flowのメトリクスを実証する。
特に、我々は出現と動きの2つの軸について2つについて結果を分けて検証する。

出現モデルについては、局所エッジ強度によってテクスチャ領域を識別する。
動きモデルについては、[25]で提案されているflowマップに従って大きな動き領域を識別する。
図6はmulti-scale voxel flowを含めた場合と含めない場合の、UCF-101テストセットにおけるPSNRパフォーマンスの比較である。
図6(b)に示すように、multi-scale アーキテクチャはDVFでより大きな動きを扱うことができるようになっている。
大きな動きは、ダウンサンプリング後の小さな動きとなり、これらの動き推定値は我々のネットワークの最終層における高解像度の推定値とミックスされる。
グラフはmulti-scale アーキテクチャが大きな動き領域についてより大きな恩恵を与えていることを示している。

我々はまた、skip connectionの有効性についても検証している。
直感的に、大きな空間解像度を持つ低層から特徴マップを連結することは、ネットワークが出力値の中により空間的なディテールを回復する手助けとなる。
この主張を確かめるため、skip connectionをなくした検証を行うと、PSNRは1.1dBとなった。

4.2　見た目の合成への一般化
ここでは、再学習することなくDVFが見た目の合成に対して一般化できることを実証する。

中間の中間ビューのみを生成することに注意しつつ、UCF-101で学習したモデルをビュー合成タスクに直接適用する一方で、一般的なビュー合成技術は任意の視点をレンダリングできる。

KITTIオドメトリデータセット[9]は、[38]でフォローされているようにここでは評価用に使用される。

表１の右に異なる手法による性能の比較をリストする。
驚くことに、ファインチューニングなしで、我々の手法はすでに[30]と[38]の0.164と0.135をともに超えている。

KITTIトレーニングセットのファインチューニングが再構成エラーをさらに軽減させることができるということが分かった。

KITTIデータセットは、もともとの学習データとは大きく異なる大きなカメラモーションを示していることに注意する。
(UCF-101は主に人間の動きに焦点を当てている)

この観察は、ボクセルフローが優れた一般化性能を持ち、ユニバーサルフレーム/ビュー合成として使用できることを意味している。

図7に定性的な比較を示す。

4.3　Self-supervisionとしてのフレーム合成
ビデオの内挿/外挿の品質を上げることに加えて、我々はビデオフレーム合成が表現学習のためのself-supervisionタスクとして機能することを実証する。

ここでは、DVFによって学習された内部表現は、教師なしフロー推定やアクション認識の事前学習に適用される。

As Unsupervised Flow Estimation
3Dボクセルフローを図2(e)に記載のように2Dモーションフィールドに投影できること再度考える。

投影した2Dモーションフィールドと正解のオプティカルフローフィールドを比較くすることによってDVFのフローフィールドを数値的に評価する。

KITTIフロー2012データセット[9]うぃテストデータセットとして使用する。
表2の左にすべてのラベル付きピクセルの平均エンドポイントエラー(EPE)を示す。
ファインチューニングののち、DVFによって生成された教師なしフローは、従来の手法[2]を超え、いくつかの教師ありディープモデル[5]に匹敵する性能を発揮している。

大規模ビデオコーパスの合成フレームを学習することで我々のモデルに必須な動き情報をエンコードすることができる。

As Unsupervised Represetation Learning
ここでは、DVFの再構成レイヤーを分類レイヤー(例えば、全結合＋ソフトマックスLossレイヤー)に置き換える。

このモデルは、UCF-101データセット(split-1)[27]の動き認識損失によってファインチューニング及びテストされている。

これは事前学習タスクとしてボクセルフローによるフレーム合成を使用することと同等である。


表2の右に示すように、我々のアプローチはlarge marginによるrandom initializationの性能を上回っており、ほかの表現学習の選択肢[33]に対して最高のパフォーマンスを示している。

ボクセルフローを用いたフレームの合成のためにDVFは出現と動きの両方に対してエンコードされなければならず、これは暗黙的に2ストリームのCNNを模倣している。


4.4　アプリケーション
DVFはHDビデオのスローモーション効果を生成するために使用することができる。
我々は現実世界のベンチマークとしてwebから様々なコンテンツや動きのタイプのHDビデオ(1080x720,30fps)のビデオを集めた。
他のフレームを削除して正解フレームとして使用した。
ここで使用されているモデルは、UCF-101データセットによってのみ、ほかの更なる適応を行わず学習されていることに注意する。
DVFは完全に畳み込みであるため、任意のサイズのビデオに適用することができる。
更なるビデオの定性的な比較は我々のプロジェクトページにて利用可能である。

Visual Comparison
既存のビデオスローモーションソフトウェアはフレーム間で生成される明らかなオプティカルフロー推定に頼っている。
したがって、我々はEpicFlow[25]を強力なベースラインとして機能させる。

図8にはそれぞれThrowとStreetのシーンのスローモーション効果について示している。
いずれの技術でも空間的に一貫した結果を作り出しているが、我々の手法のほうがより性能が良い。
例えば、Throwのシーンについては、DVFはロゴの構造を残しており、また、Streetのシーンについても、DVFは歩行者と広告のオクルージョンをよりよく扱えている。

しかしながら、時間方向での実験結果を見た時にはさらに利点が強調される。

この利点にてついて、補間画像のxtスライスによって静的に表す（図8(c)）

EpicFlowの結果は、時間方向でギザギザしている。
EpicFlowは、空間的な一貫性はあるものの時間的に不連続な複雑な動きに対しては長さゼロのフローベクトルを生成することが多かった。

DeepLearningは一般的に、オプティカルフローベクトルの線形スケーリングよりもより時間的に滑らかな結果を生成することができる。

UserStudy
異なる手法の定性的な比較を客観的に行うため、最終的なスローモーションビデオについてユーザースタディをお行った。
我々はDVFに対して、EpicFlowと正解画像を比較した。
side-by-sideの比較によって、2つの異なる手法による合成ビデオはダイアゴナルスプリットで合成した。図9(a)
左右の位置はランダムにおかれている。
ユーザーが今までに見たことのない20シーンがユーザースタディとして使用された。
ユーザーに対しては10の合成ビデオについてより好みのほうを選んでもらった。
例えば、左右のどちらがより好みであるかといったように。

図9(b)に示すように、我々のアプローチはテストシーケンスにおいて、EpicFlowよりも優位に優れている。
帰無仮説において、「EpicFlowと我々の結果に違いはない」という仮説は、p値はp<0.00001であるため、仮説は棄却される。
さらに、シーケンスの半分について、参加者は正解画像とほぼ同じ頻度で我々の手法の結果を選択しており、視覚的な品質が等しいことを示唆している。

帰無仮説において、「正解画像と我々の結果に違いはない」という仮説は、p値が0.838193であるため、今回のケースでは統計的に優位に帰無仮説を棄却できないということ人なる。

全体として、DVFは幅広い動画にわたって、高品質なスローモーション効果を生成することができると結論する。


Failure Cases
DVFの典型的な失敗パターンは(Parkのシーンのような)繰り返しパターンのシーンである。
この場合、RGBの違いを参照するだけで、コピーする正解のソースボクセルを決定することは難しい。

この問題に対処するためには、より強力な一般化項を追加する方法が考えられる。

5．Discussion
本論文では、動画フレーム合成のためにend-to-endのディープネットワークであるDVFを提案した。我々の手法では、ゼロからそれらをハルキネートするのではなく、存在する動画フレームからピクセルをコピーすることができた。

一方で、我々の手法では、どんな動画を用いても教師なしの手順で学習することができる。
実験では、このアプローチが動画の内挿と外挿においてオプティカルフローと最近のCNN技術の両方を改善することを示した。

将来的には、ほかのビデオフレームからコピー不可能なピクセルをよりよく予測するために、純粋な合成レイヤーにフローレイヤーを組み合わせる方法が考えられる。
また、我々の手法をマルチフレーム予測に拡張することはかなり簡単である。
目的の時間的なステップをネットワークの入力として使用する(例えば3つの内挿フレームではt=.25とする)といったようないくつかの面白い選択肢がある。


我々のネットワークを圧縮してモバイルデバイスで実行できるようにすることもまた我々が探求したい方向性である。